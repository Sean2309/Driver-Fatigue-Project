{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Dataset Information\n",
    "\n",
    "\n",
    "Dataset Link : https://www.kaggle.com/datasets/ismailnasri20/driver-drowsiness-dataset-ddd/data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to consolidate image paths\n",
    "def consolidate_image_paths(input_path : str, subfolder_name: str = \"\") -> list[str]:\n",
    "    return [os.path.join(input_path, subfolder_name, p) for p in os.listdir(os.path.join(input_path, subfolder_name))]\n",
    "\n",
    "# Helper function to map image paths to labels\n",
    "def map_image_paths_to_labels(image_paths: list[str], label: int) -> dict:\n",
    "    return {p: label for p in image_paths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of Dataset 2 ============================================================================================================\n",
    "base_path_2 = \"./Datasets/Dataset_2/\"\n",
    "\n",
    "# Adds the relative paths of all the images in the dataset\n",
    "drowsy_paths = consolidate_image_paths(base_path_2, \"Drowsy\")\n",
    "non_drowsy_paths = consolidate_image_paths(base_path_2, \"Non Drowsy\")\n",
    "\n",
    "# Combining all the paths\n",
    "all_paths = drowsy_paths + non_drowsy_paths\n",
    "\n",
    "# # Mapping the image paths to their respective labels\n",
    "drowsy_labels = map_image_paths_to_labels(drowsy_paths, 1)\n",
    "non_drowsy_labels = map_image_paths_to_labels(non_drowsy_paths, 0)\n",
    "\n",
    "# # Combining all the labels\n",
    "all_labels = {**drowsy_labels, **non_drowsy_labels}\n",
    "\n",
    "print(f\"Total Number of Images: {len(all_paths)}\")\n",
    "print(f\"Difference between Drowsy and Non-Drowsy: {len(drowsy_paths) - len(non_drowsy_paths)}\")\n",
    "\n",
    "# TODO : Add some visuals in seaborn to show the distribution of the labels\n",
    "# End of Dataset 2 ============================================================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights from Dataset 2\n",
    "\n",
    "- There are `2903` drowsy images than non-drowsy images.\n",
    "- The dataset is imbalanced.\n",
    "- To balance the dataset, we can consider several techniques:\n",
    "    - Oversampling\n",
    "    - Undersampling\n",
    "    - Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "### `Steps`:\n",
    "1. Image Resizing\n",
    "2. Data Splitting\n",
    "3. Reshuffling\n",
    "4. Undersampling (Majority Class)\n",
    "5. Data Augmentation (for training data)\n",
    "6. Data Normalization\n",
    "\n",
    "### Preprocessing Steps Methodology\n",
    "1. **Image Resizing**:\n",
    "    - The images should be resized first to ensure all images are of the same dimensions.\n",
    "    - The images are resized to `224x224` pixels.\n",
    "\n",
    "2. **Data Splitting**:\n",
    "    - Dataset should be split before any form of augmentation or sampling to ensure that the model is evaluated on unseen data.\n",
    "    - Augmented data can be spilt into the testing and validation sets otherwise.\n",
    "    - The dataset is split into `70%` training, `15%` validation and `15%` testing sets.\n",
    "\n",
    "3. **Reshuffling**:\n",
    "    - The dataset is reshuffled to ensure that the data is not ordered in any way.\n",
    "    - This helps to prevent the model from learning any patterns in the data that may not be present in real-world scenarios.\n",
    "\n",
    "4. **Undersampling**:\n",
    "    - The majority class is undersampled to the number of images in the minority class.\n",
    "\n",
    "5. **Data Augmentation**:\n",
    "    - Data Augmentation is applied to the training set only to increase the variability of the training data. \n",
    "    - This helps to prevent overfitting and help to contextualise to real-world scenarios. \n",
    "    - Possible augmentations are:\n",
    "        - Rotation\n",
    "        - Horizontal Flip\n",
    "        - Vertical Flip\n",
    "        - Increasing the brightness\n",
    "\n",
    "6. **Data Normalization**:\n",
    "    - The pixel values are normalized to the range `[0, 1]` by dividing by `255`.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Resizing. Default resized dim : 224x224\n",
    "def resize_image(image_path : str, size : tuple[int, int] = (224, 224)) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Resizes an image to the specified size and returns the resized image as an ndarray.\n",
    "    Handles potential errors if the image cannot be loaded.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Checking if image exists\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Error reading image from path: {image_path}\")\n",
    "    \n",
    "    # Convert from BGR to RGB to ensure proper color representation\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_image = cv2.resize(image_rgb, size)\n",
    "    \n",
    "    return resized_image\n",
    "\n",
    "# Function to save the resized image\n",
    "def save_resized_image(image: np.ndarray, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Converts the image back to BGR (if needed) and saves it to the specified output path.\n",
    "    \"\"\"\n",
    "    # Convert back to BGR before saving with OpenCV (if required for consistency)\n",
    "    image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Save the image\n",
    "    cv2.imwrite(output_path, image_bgr)\n",
    "\n",
    "# Data Splitting\n",
    "def split_data(X, y): # will the X and y be a dictionary or a list?\n",
    "    \n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Shuffle Image Paths\n",
    "def shuffle_paths(X_train: list[str], y_train: list[int]) -> tuple[list[tuple[str, int]], list[tuple[str, int]]]:\n",
    "    \"\"\"\n",
    "    Separates and shuffles the training paths by label (drowsy and non-drowsy).\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (list[str]): List of training image paths.\n",
    "        y_train (list[int]): List of labels corresponding to each image path.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Two lists containing shuffled (path, label) tuples for each class.\n",
    "    \"\"\"\n",
    "    # Separate paths by label in training data\n",
    "    drowsy_train = [(path, label) for path, label in zip(X_train, y_train) if label == 1]\n",
    "    non_drowsy_train = [(path, label) for path, label in zip(X_train, y_train) if label == 0]\n",
    "\n",
    "    # Shuffle each class independently\n",
    "    np.random.shuffle(drowsy_train)\n",
    "    np.random.shuffle(non_drowsy_train)\n",
    "\n",
    "    return drowsy_train, non_drowsy_train\n",
    "\n",
    "# Undersampling the majority class\n",
    "def undersample_majority_class(drowsy_data: list[tuple], non_drowsy_data: list[tuple]) -> list[tuple]:\n",
    "    \"\"\"\n",
    "    Undersamples the majority class to match the size of the minority class.\n",
    "    \"\"\"\n",
    "    # Determine the size for undersampling\n",
    "    undersample_size = min(len(drowsy_data), len(non_drowsy_data))\n",
    "\n",
    "    # Undersample the majority class\n",
    "    if len(non_drowsy_data) > len(drowsy_data):\n",
    "        non_drowsy_data = non_drowsy_data[:undersample_size]\n",
    "    else:\n",
    "        drowsy_data = drowsy_data[:undersample_size]\n",
    "\n",
    "    # Combine both classes and shuffle the final training data\n",
    "    balanced_train = drowsy_data + non_drowsy_data\n",
    "    np.random.shuffle(balanced_train)\n",
    "    \n",
    "    return balanced_train\n",
    "\n",
    "# Data Augmentation : Rotation, \n",
    "def augment_and_save_images(balanced_train_data, save_dir, target_size=(224, 224), augment_count=5):\n",
    "    \"\"\"\n",
    "    Applies data augmentation to each image in the dataset and saves the augmented images.\n",
    "    \n",
    "    Parameters:\n",
    "        balanced_train_data (list): List of (image_path, label) tuples for balanced training data.\n",
    "        save_dir (str): Directory where augmented images will be saved.\n",
    "        target_size (tuple): Target size for resizing images.\n",
    "        augment_count (int): Number of augmented images to generate per original image.\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize ImageDataGenerator with augmentation parameters\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=15,\n",
    "        brightness_range=[0.8, 1.2],\n",
    "        horizontal_flip=True,\n",
    "        zoom_range=0.1,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    \n",
    "    # Clear the save directory if it already exists\n",
    "    if os.path.exists(save_dir):\n",
    "        shutil.rmtree(save_dir)\n",
    "    os.makedirs(os.path.join(save_dir, \"Drowsy\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir, \"Non Drowsy\"), exist_ok=True)\n",
    "    \n",
    "    # Process each image in the balanced training data\n",
    "    for image_path, label in balanced_train_data:\n",
    "        # Load and preprocess the image\n",
    "        img = load_img(image_path, target_size=target_size)\n",
    "        img_array = img_to_array(img)  # Convert to array\n",
    "        img_array = img_array.reshape((1,) + img_array.shape)  # Reshape for generator\n",
    "\n",
    "        # Determine save path based on label\n",
    "        class_dir = \"Drowsy\" if label == 1 else \"Non Drowsy\"\n",
    "        save_path = os.path.join(save_dir, class_dir)\n",
    "\n",
    "        # Generate and save augmented images\n",
    "        i = 0\n",
    "        for batch in datagen.flow(img_array, batch_size=1, save_to_dir=save_path, \n",
    "                                  save_prefix=os.path.basename(image_path).split('.')[0], \n",
    "                                  save_format='jpg'):\n",
    "            i += 1\n",
    "            if i >= augment_count:\n",
    "                break  # Stop after saving the specified number of augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Resizing\n",
    "output_folder_drowsy = os.path.join(base_path_2, \"Resized_Images\", \"Drowsy\")\n",
    "output_folder_non_drowsy = os.path.join(base_path_2, \"Resized_Images\", \"Non Drowsy\")\n",
    "\n",
    "# Loop through all the images and resize them with tqdm progress bar\n",
    "# for image_path, label in tqdm(all_labels.items(), desc=\"Resizing Images\", ncols=100):\n",
    "#     try:\n",
    "#         resized_image = resize_image(image_path, (224, 224))\n",
    "\n",
    "#         # Save in respective folders based on the label\n",
    "#         if label == 1:  # Drowsy\n",
    "#             output_file_name = os.path.basename(image_path)\n",
    "#             output_path = os.path.join(output_folder_drowsy, output_file_name)\n",
    "#         else:  # Non-Drowsy\n",
    "#             output_file_name = os.path.basename(image_path)\n",
    "#             output_path = os.path.join(output_folder_non_drowsy, output_file_name)\n",
    "\n",
    "#         save_resized_image(resized_image, output_path)\n",
    "\n",
    "#     except ValueError as e:\n",
    "#         print(f\"Skipping image {image_path}: {e}\")\n",
    "\n",
    "# # Verify the number of resized images\n",
    "# print(f\"Image Number Difference: {len(os.listdir(output_folder_drowsy)) + len(os.listdir(output_folder_non_drowsy)) - len(all_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "\n",
    "# Consolidate the paths of all resized images\n",
    "drowsy_paths = consolidate_image_paths(output_folder_drowsy)\n",
    "non_drowsy_paths = consolidate_image_paths(output_folder_non_drowsy)\n",
    "\n",
    "# Create the labels for each class\n",
    "drowsy_labels = [1] * len(drowsy_paths)\n",
    "non_drowsy_labels = [0] * len(non_drowsy_paths)\n",
    "\n",
    "# Combine the data and labels\n",
    "all_images = drowsy_paths + non_drowsy_paths\n",
    "all_labels = drowsy_labels + non_drowsy_labels\n",
    "\n",
    "# Split the data into train, val, and test sets\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(all_images, all_labels)\n",
    "\n",
    "# Convert the splits back to dictionaries using the helper function\n",
    "train_dict = map_image_paths_to_labels(X_train, 1) if X_train else {}\n",
    "val_dict = map_image_paths_to_labels(X_val, 1) if X_val else {}\n",
    "test_dict = map_image_paths_to_labels(X_test, 1) if X_test else {}\n",
    "\n",
    "# Example: Print the first few training image paths and their labels\n",
    "print(\"Training Data (First 5):\", list(train_dict.items())[:5])\n",
    "print(\"Validation Data (First 5):\", list(val_dict.items())[:5])\n",
    "print(\"Test Data (First 5):\", list(test_dict.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m balanced_train_data \u001b[38;5;241m=\u001b[39m undersample_majority_class(drowsy_train, non_drowsy_train)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Augmented Data Generator\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Assuming `balanced_train_data` is a list of (image_path, label) tuples\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43maugment_and_save_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbalanced_train_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./augmented_train_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 126\u001b[0m, in \u001b[0;36maugment_and_save_images\u001b[1;34m(balanced_train_data, save_dir, target_size, augment_count)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# Generate and save augmented images\u001b[39;00m\n\u001b[0;32m    125\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdatagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_to_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m                          \u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m                          \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maugment_count\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\SUTD\\Term 7\\CV\\Driver-Fatigue-Project\\cvEnv\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:112\u001b[0m, in \u001b[0;36mIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    109\u001b[0m     index_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_generator)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# The transformation of images is not under thread lock\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# so it can be done in parallel\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_batches_of_transformed_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_array\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\SUTD\\Term 7\\CV\\Driver-Fatigue-Project\\cvEnv\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:654\u001b[0m, in \u001b[0;36mNumpyArrayIterator._get_batches_of_transformed_samples\u001b[1;34m(self, index_array)\u001b[0m\n\u001b[0;32m    652\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx[j]\n\u001b[0;32m    653\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_data_generator\u001b[38;5;241m.\u001b[39mget_random_transform(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m--> 654\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_data_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    657\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_data_generator\u001b[38;5;241m.\u001b[39mstandardize(x)\n\u001b[0;32m    658\u001b[0m batch_x[i] \u001b[38;5;241m=\u001b[39m x\n",
      "File \u001b[1;32mc:\\SUTD\\Term 7\\CV\\Driver-Fatigue-Project\\cvEnv\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:1413\u001b[0m, in \u001b[0;36mImageDataGenerator.apply_transform\u001b[1;34m(self, x, transform_parameters)\u001b[0m\n\u001b[0;32m   1410\u001b[0m img_col_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol_axis \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1411\u001b[0m img_channel_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel_axis \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1413\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mapply_affine_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform_parameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtheta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform_parameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform_parameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1418\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform_parameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform_parameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform_parameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrow_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_row_axis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcol_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_col_axis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchannel_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_channel_axis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfill_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1426\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1427\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transform_parameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannel_shift_intensity\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1430\u001b[0m     x \u001b[38;5;241m=\u001b[39m apply_channel_shift(\n\u001b[0;32m   1431\u001b[0m         x,\n\u001b[0;32m   1432\u001b[0m         transform_parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannel_shift_intensity\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1433\u001b[0m         img_channel_axis,\n\u001b[0;32m   1434\u001b[0m     )\n",
      "File \u001b[1;32mc:\\SUTD\\Term 7\\CV\\Driver-Fatigue-Project\\cvEnv\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:1880\u001b[0m, in \u001b[0;36mapply_affine_transform\u001b[1;34m(x, theta, tx, ty, shear, zx, zy, row_axis, col_axis, channel_axis, fill_mode, cval, order)\u001b[0m\n\u001b[0;32m   1876\u001b[0m final_affine_matrix \u001b[38;5;241m=\u001b[39m transform_matrix[:\u001b[38;5;241m2\u001b[39m, :\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m   1877\u001b[0m final_offset \u001b[38;5;241m=\u001b[39m transform_matrix[:\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m   1879\u001b[0m channel_images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m-> 1880\u001b[0m     \u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maffine_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_channel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfinal_affine_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfinal_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1888\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x_channel \u001b[38;5;129;01min\u001b[39;00m x\n\u001b[0;32m   1889\u001b[0m ]\n\u001b[0;32m   1890\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(channel_images, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   1891\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrollaxis(x, \u001b[38;5;241m0\u001b[39m, channel_axis \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\SUTD\\Term 7\\CV\\Driver-Fatigue-Project\\cvEnv\\Lib\\site-packages\\scipy\\ndimage\\_interpolation.py:626\u001b[0m, in \u001b[0;36maffine_transform\u001b[1;34m(input, matrix, offset, output_shape, output, order, mode, cval, prefilter)\u001b[0m\n\u001b[0;32m    623\u001b[0m     _nd_image\u001b[38;5;241m.\u001b[39mzoom_shift(filtered, matrix, offset\u001b[38;5;241m/\u001b[39mmatrix, output, order,\n\u001b[0;32m    624\u001b[0m                          mode, cval, npad, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 626\u001b[0m     \u001b[43m_nd_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeometric_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m                                  \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Reshuffling\n",
    "drowsy_train, non_drowsy_train = shuffle_paths(X_train, y_train)\n",
    "\n",
    "# Undersampling\n",
    "balanced_train_data = undersample_majority_class(drowsy_train, non_drowsy_train)\n",
    "\n",
    "# Augmented Data Generator\n",
    "# Assuming `balanced_train_data` is a list of (image_path, label) tuples\n",
    "augment_and_save_images(balanced_train_data, save_dir=os.path.join(base_path_2, './Augmented_Train_Data)'), augment_count=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
